from google import genai
from google.genai import types
import utils
import core_rfi 
import prompts

def get_client(api_key):
    return genai.Client(api_key=api_key)

def extract_structure(api_key, structure_file):
    try:
        client = get_client(api_key)
        file_text = utils.parse_uploaded_file(structure_file)
        prompt = f"{prompts.LOGIC_PROMPTS['structure_extraction']}\n[íŒŒì¼ ë‚´ìš©]\n{file_text[:15000]}"
        resp = client.models.generate_content(model="gemini-3-flash-preview", contents=prompt)
        return resp.text
    except Exception as e:
        return f"êµ¬ì¡° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}"

def parse_all_files(uploaded_files, read_content=True):
    all_text = ""
    file_list_str = ""
    if uploaded_files:
        for file in uploaded_files:
            file_list_str += f"- {file.name}\n"
            if read_content:
                parsed = utils.parse_uploaded_file(file)
                all_text += parsed
    
    if not read_content:
        all_text = "(RFI ëª¨ë“œ: ë‚´ìš©ì„ ì½ì§€ ì•ŠìŒ)"
        
    return all_text, file_list_str

def get_default_structure(template_key):
    return prompts.TEMPLATE_STRUCTURES.get(template_key, "")

def generate_report_stream(api_key, model_name, inputs, thinking_level, file_context):
    client = get_client(api_key)
    template_opt = inputs['template_option']
    
    # [RFI Mode]
    if template_opt == 'rfi':
        stream = core_rfi.generate_rfi_stream(api_key, model_name, inputs, thinking_level)
        for chunk in stream:
            yield chunk
        return

    # [PPT Mode]
    if template_opt == 'presentation':
        system_instruction = prompts.LOGIC_PROMPTS['ppt_system']
        main_prompt = f"""
        [System: Thinking Level {thinking_level.upper() if isinstance(thinking_level, str) else 'HIGH'}]
        [ìŠ¬ë¼ì´ë“œ êµ¬ì¡°] {inputs['structure_text']}
        [ë§¥ë½] {inputs['context_text']}
        [ë°ì´í„°] {file_context[:50000]}
        """
        config = types.GenerateContentConfig(
            max_output_tokens=65536,
            temperature=0.7,
            system_instruction=system_instruction
        )

    # [Custom Mode] - ì„œì‹ ë³µì œ
    elif template_opt == 'custom':
        system_instruction = prompts.LOGIC_PROMPTS['custom_system']
        main_prompt = f"""
        [System: Thinking Level {thinking_level.upper() if isinstance(thinking_level, str) else 'HIGH'}]
        
        [ëª©í‘œ ë¬¸ì„œ êµ¬ì¡° (ë°˜ë“œì‹œ ì¤€ìˆ˜)]
        {inputs['structure_text']}
        
        [ì „ì²´ ë§¥ë½]
        {inputs['context_text']}
        
        [ë³¸ë¬¸ ì±„ìš°ê¸°ìš© ë¶„ì„ ë°ì´í„°]
        {file_context[:50000]}
        """
        config = types.GenerateContentConfig(
            max_output_tokens=65536,
            temperature=0.5, # êµ¬ì¡° ì¤€ìˆ˜ë¥¼ ìœ„í•´ ì•½ê°„ ë‚®ì¶¤
            system_instruction=system_instruction
        )

    # [Standard Report Mode]
    else:
        system_instruction = prompts.LOGIC_PROMPTS['report_system']
        if template_opt == 'simple_review':
             system_instruction += "\n**ì¤‘ìš”: 10í˜ì´ì§€ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.**"
        if inputs['use_diagram']:
            system_instruction += "\n**ë„ì‹í™”**: í•„ìš”ì‹œ {{DIAGRAM: ì„¤ëª…}} íƒœê·¸ ì‚½ì…."

        main_prompt = f"""
        [System: Thinking Level {thinking_level.upper() if isinstance(thinking_level, str) else 'HIGH'}]
        [Critical Instruction] Analyze the provided data deeply and step-by-step. Prioritize accuracy and logical consistency.
        [ë¬¸ì„œ êµ¬ì¡°] {inputs['structure_text']}
        [ë§¥ë½] {inputs['context_text']}
        [ë°ì´í„°] {file_context[:50000]}
        """
        
        tools = []
        if "ë‰´ìŠ¤" in inputs['structure_text'] or "ë™í–¥" in inputs['structure_text']:
            tools = [types.Tool(google_search=types.GoogleSearch())]

        config = types.GenerateContentConfig(
            tools=tools,
            max_output_tokens=8192,
            temperature=0.3,
            system_instruction=system_instruction
        )

    response_stream = client.models.generate_content_stream(
        model=model_name,
        contents=main_prompt,
        config=config
    )
    for chunk in response_stream:
        yield chunk

def generate_report_stream_chained(api_key, model_name, inputs, thinking_level, file_context):
    """3ë‹¨ê³„ Chained Promptingìœ¼ë¡œ íˆ¬ìì‹¬ì‚¬ë³´ê³ ì„œ ìƒì„± (í’ˆì§ˆ ìš°ì„ )"""
    client = get_client(api_key)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê³µí†µ)
    system_instruction = prompts.LOGIC_PROMPTS['report_system_base']
    if inputs.get('use_diagram'):
        system_instruction += "\n**ë„ì‹í™”**: í•„ìš”ì‹œ {{DIAGRAM: ì„¤ëª…}} íƒœê·¸ ì‚½ì…."

    # 3ê°œ íŒŒíŠ¸ ì •ì˜ (part_key, title, max_tokens)
    parts = [
        ('report_part1', 'Part 1/3: Executive Summary & Investment Highlights', 65536),
        ('report_part2', 'Part 2/3: Target Company & Market Analysis', 65536),
        ('report_part3', 'Part 3/3: Financials, Valuation, Risk & ì¢…í•©ì˜ê²¬', 65536)
    ]

    accumulated_result = ""

    for part_key, part_title, max_tokens in parts:
        # ì§„í–‰ ìƒí™© ì•Œë¦¼
        status_text = f"\n\n---\n\nğŸ“ **[{part_title}] ìƒì„± ì¤‘...**\n\n"
        yield types.GenerateContentResponse(
            candidates=[types.Candidate(
                content=types.Content(parts=[types.Part(text=status_text)])
            )]
        )

        # ì´ì „ íŒŒíŠ¸ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
        prev_context = ""
        if accumulated_result:
            prev_context = f"""
[ì´ì „ ì‘ì„± ë‚´ìš© - ì°¸ê³ ìš©, ì¤‘ë³µ ì‘ì„± ê¸ˆì§€]
{accumulated_result[-20000:]}
"""

        main_prompt = f"""
[System: Thinking Level {thinking_level.upper() if isinstance(thinking_level, str) else 'HIGH'}]
[Critical Instruction] Analyze the provided data deeply and step-by-step. Prioritize accuracy and logical consistency.

{prev_context}

{prompts.LOGIC_PROMPTS[part_key]}

[ë§¥ë½]
{inputs['context_text']}

[ë¶„ì„ ë°ì´í„°]
{file_context[:45000]}
"""

        tools = []
        # Part 2 (ì‹œì¥ ë¶„ì„)ì—ì„œ ì›¹ ê²€ìƒ‰ í™œì„±í™”
        if part_key == 'report_part2':
            tools = [types.Tool(google_search=types.GoogleSearch())]

        config = types.GenerateContentConfig(
            tools=tools,
            max_output_tokens=max_tokens,
            temperature=0.3,
            system_instruction=system_instruction
        )

        part_result = ""
        response_stream = client.models.generate_content_stream(
            model=model_name,
            contents=main_prompt,
            config=config
        )

        for chunk in response_stream:
            if chunk.text:
                part_result += chunk.text
            yield chunk

        accumulated_result += part_result


def refine_report(api_key, model_name, current_text, refine_query):
    client = get_client(api_key)
    refine_prompt = f"""
    ë‹¹ì‹ ì€ ë¬¸ì„œ ìˆ˜ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ì ìš”ì²­: "{refine_query}"
    ê¸°ì¡´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **"## ğŸ”„ ì¶”ê°€ ìš”ì²­ ë°˜ì˜"** í•˜ìœ„ì— ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.
    [ê¸°ì¡´ ë‚´ìš©] {current_text[:20000]}...
    """
    resp = client.models.generate_content(model=model_name, contents=refine_prompt)
    return resp.text