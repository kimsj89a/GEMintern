import streamlit as st
import pandas as pd
import requests
import time
from urllib.parse import urljoin
import core_logic

# BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

def render_crawler_panel(settings):
    """ì›¹ í¬ë¡¤ëŸ¬ UI íŒ¨ë„ (Internal)"""
    st.markdown("### ğŸŒ ì›¹ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ (Web Crawler)")
    
    if not BS4_AVAILABLE:
        st.error("âŒ `beautifulsoup4` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.code("pip install beautifulsoup4 requests", language="bash")
        return
    
    st.markdown(f"""
        <div style='background-color: #f0f2f6; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #0068c9;'>
        <h4 style='margin-top: 0; color: #0068c9;'>ğŸ•·ï¸ ë‚´ì¥ í¬ë¡¤ëŸ¬ (Built-in)</h4>
        ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ì˜ì¡´ì„± ì—†ì´, URLì„ ì…ë ¥í•˜ë©´ ì¦‰ì‹œ í¬ë¡¤ë§í•˜ì—¬ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
        </div>
    """, unsafe_allow_html=True)

    # íƒ­ êµ¬ì„±
    tab_run, tab_view = st.tabs(["ğŸš€ í¬ë¡¤ë§ ì‹¤í–‰", "ğŸ“Š ê²°ê³¼ ë¶„ì„"])

    with tab_run:
        st.markdown("#### í¬ë¡¤ë§ íŒŒë¼ë¯¸í„° ì„¤ì •")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            target_urls_input = st.text_area("Target URLs (í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥)", placeholder="https://www.example.com\nhttps://www.google.com", height=100)
        with col2:
            depth = st.number_input("Depth (ë§í¬ ì¶”ì  ê¹Šì´)", min_value=1, max_value=3, value=1, help="ë„ˆë¬´ ê¹Šê²Œ ì„¤ì •í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
            max_pages = st.number_input("Max Pages (URLë‹¹ ìµœëŒ€)", min_value=1, max_value=50, value=5)
            
        if st.button("ğŸ•·ï¸ í¬ë¡¤ë§ ì‹œì‘", use_container_width=True, type="primary"):
            if not target_urls_input.strip():
                st.warning("âš ï¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                urls = [url.strip() for url in target_urls_input.split('\n') if url.strip()]
                st.info(f"ğŸ“¡ ì´ {len(urls)}ê°œì˜ ì‹œì‘ URLì— ëŒ€í•´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                
                all_results = []
                progress_bar = st.progress(0)
                
                for i, start_url in enumerate(urls):
                    # ë‚´ë¶€ í¬ë¡¤ë§ ë¡œì§ ì‹¤í–‰
                    df_res = _crawl_internal(start_url, depth, max_pages)
                    all_results.append(df_res)
                    progress_bar.progress((i + 1) / len(urls))
                
                if all_results:
                    final_df = pd.concat(all_results, ignore_index=True)
                    st.session_state['crawled_data'] = final_df
                    st.success(f"ğŸ‰ ì™„ë£Œ! ì´ {len(final_df)}ê°œì˜ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    st.rerun() # ê²°ê³¼ íƒ­ ê°±ì‹ ì„ ìœ„í•´ ë¦¬ëŸ°

    with tab_view:
        st.markdown("#### ìˆ˜ì§‘ ë°ì´í„° ë·°ì–´")
        
        if 'crawled_data' in st.session_state and not st.session_state['crawled_data'].empty:
            df = st.session_state['crawled_data']
            st.dataframe(df, use_container_width=True)
            st.caption(f"ğŸ“Š ì´ {len(df)}í–‰")
            
            col1, col2 = st.columns(2)
            with col1:
                # CSV ë‹¤ìš´ë¡œë“œ
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                    csv,
                    "crawled_results.csv",
                    "text/csv",
                    key='download-csv',
                    use_container_width=True
                )
            with col2:
                # TXT ë‹¤ìš´ë¡œë“œ (ì „ì²´ ë‚´ìš© ë³´ì¡´)
                txt_output = ""
                for _, row in df.iterrows():
                    txt_output += f"Title: {row.get('title', 'No Title')}\n"
                    txt_output += f"URL: {row.get('url', 'No URL')}\n"
                    txt_output += f"Content:\n{row.get('content', '')}\n"
                    txt_output += "="*80 + "\n\n"
                
                st.download_button(
                    "ğŸ“¥ TXT ë‹¤ìš´ë¡œë“œ (ì „ì²´ ë‚´ìš©)",
                    txt_output,
                    "crawled_results.txt",
                    "text/plain",
                    key='download-txt',
                    use_container_width=True
                )

            # Gemini ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            st.markdown("---")
            st.markdown("#### ğŸ¤– AI ìš”ì•½ ë³´ê³ ì„œ ì‘ì„±")
            
            api_key = settings.get("api_key")
            if not api_key:
                st.warning("âš ï¸ ìƒë‹¨ ì„¤ì •ì—ì„œ Google API Keyë¥¼ ì…ë ¥í•˜ë©´ ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                if st.button("ğŸ“ ìˆ˜ì§‘ ë°ì´í„° ìš”ì•½í•˜ê¸°", type="primary", use_container_width=True):
                    with st.spinner("Geminiê°€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                        try:
                            # í…ìŠ¤íŠ¸ í†µí•©
                            combined_text = ""
                            for _, row in df.iterrows():
                                combined_text += f"Title: {row.get('title', '')}\nURL: {row.get('url', '')}\nContent:\n{row.get('content', '')}\n\n"
                            
                            # Gemini í˜¸ì¶œ
                            client = core_logic.get_client(api_key)
                            model_name = settings.get("model_name", "gemini-3-pro-preview")
                            
                            prompt = f"""
                            ë‹¹ì‹ ì€ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì›¹ í¬ë¡¤ë§ì„ í†µí•´ ìˆ˜ì§‘ëœ ë°ì´í„°ì…ë‹ˆë‹¤.
                            ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
                            
                            [ì‘ì„± í˜•ì‹]
                            1. Executive Summary (ìš”ì•½)
                            2. ì£¼ìš” ìˆ˜ì§‘ ë‚´ìš© ë° íŒ©íŠ¸ ì •ë¦¬
                            3. ì¸ì‚¬ì´íŠ¸ ë° ì‹œì‚¬ì 
                            
                            [ìˆ˜ì§‘ ë°ì´í„°]
                            {combined_text[:500000]}
                            """
                            
                            response = client.models.generate_content(
                                model=model_name,
                                contents=prompt
                            )
                            
                            st.markdown("### ğŸ“„ ìš”ì•½ ë³´ê³ ì„œ")
                            st.container(border=True).markdown(response.text)
                            
                        except Exception as e:
                            st.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.info("ğŸ“­ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'í¬ë¡¤ë§ ì‹¤í–‰' íƒ­ì—ì„œ ì‘ì—…ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")

def _crawl_internal(start_url, max_depth, max_pages):
    """ì‹¤ì œ í¬ë¡¤ë§ ìˆ˜í–‰ í•¨ìˆ˜"""
    visited = set()
    queue = [(start_url, 0)]
    results = []
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    with st.status(f"Processing: {start_url}") as status:
        while queue and len(visited) < max_pages:
            url, depth = queue.pop(0)
            if url in visited: continue
            visited.add(url)
            
            status.update(label=f"Fetching ({len(visited)}/{max_pages}): {url}")
            
            try:
                resp = requests.get(url, headers=headers, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    title = soup.title.string.strip() if soup.title else url
                    text = soup.get_text(separator=' ', strip=True)
                    
                    results.append({
                        "url": url,
                        "title": title,
                        "depth": depth,
                        "content": text[:2000] + "..." if len(text) > 2000 else text
                    })
                    
                    if depth < max_depth:
                        for link in soup.find_all('a', href=True):
                            next_url = urljoin(url, link['href'])
                            if next_url.startswith("http") and next_url not in visited:
                                queue.append((next_url, depth + 1))
                else:
                    results.append({"url": url, "title": f"Error {resp.status_code}", "depth": depth, "content": ""})
            except Exception as e:
                results.append({"url": url, "title": "Error", "depth": depth, "content": str(e)})
                
            time.sleep(0.2)
            
    return pd.DataFrame(results)