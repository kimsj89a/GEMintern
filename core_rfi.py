import os
import datetime
from google import genai
from google.genai import types
import prompts

def get_client(api_key):
    return genai.Client(api_key=api_key)

def analyze_rfi_status(client, existing_rfi, file_index_str):
    """Step 1: Flash ëª¨ë¸ë¡œ ì¸ë±ì‹±"""
    prompt = f"""
    {prompts.RFI_PROMPTS['indexing']}
    
    [ê¸°ì¡´ ìš”ì²­ ìë£Œ ëª©ë¡(RFI)]
    {existing_rfi}
    
    [ìˆ˜ë ¹í•œ íŒŒì¼ ì¸ë±ìŠ¤ (Browser Scan)]
    {file_index_str}
    """
    try:
        resp = client.models.generate_content(
            model="gemini-3-flash-preview", 
            contents=prompt,
            config=types.GenerateContentConfig(temperature=0.1)
        )
        return resp.text
    except Exception as e:
        return f"ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}"

def generate_rfi_stream(api_key, model_name, inputs, thinking_level):
    """RFI ìƒì„± ë©”ì¸ ë¡œì§"""
    client = get_client(api_key)
    
    # UIì—ì„œ ë³µì‚¬/ë¶™ì—¬ë„£ê¸° í•œ í…ìŠ¤íŠ¸ ì‚¬ìš©
    file_index_str = inputs.get('rfi_file_list_input', '')
    if not file_index_str:
        file_index_str = "(íŒŒì¼ ì¸ë±ìŠ¤ ì—†ìŒ - ì‚¬ìš©ìê°€ ì…ë ¥í•˜ì§€ ì•ŠìŒ)"
    
    yield types.GenerateContentResponse(
        candidates=[types.Candidate(
            content=types.Content(parts=[types.Part(text="ğŸ“‚ [Step 1] íŒŒì¼ ì¸ë±ìŠ¤ ê¸°ë°˜ ëŒ€ì‚¬(Indexing) ì§„í–‰ ì¤‘...\n\n")])
        )]
    )
    
    rfi_status_table = analyze_rfi_status(client, inputs['rfi_existing'], file_index_str)
    
    yield types.GenerateContentResponse(
        candidates=[types.Candidate(
            content=types.Content(parts=[types.Part(text=f"{rfi_status_table}\n\n---\nğŸ§  [Step 2] ë¶€ì¡± ìë£Œ ë¶„ì„ ë° ìµœì¢… RFI ì‘ì„± ì¤‘... ({model_name})\n\n")])
        )]
    )

    main_prompt = f"""
    [System: Thinking Level {thinking_level.upper() if isinstance(thinking_level, str) else 'HIGH'}]
    
    [1ì°¨ ìë£Œ ì ê²€ ê²°ê³¼]
    {rfi_status_table}

    [ì‚¬ìš©ì ì¶”ê°€ ì§ˆë¬¸/ë§¥ë½]
    {inputs['context_text']}
    """
    
    config = types.GenerateContentConfig(
        max_output_tokens=8192,
        temperature=0.2, 
        system_instruction=prompts.RFI_PROMPTS['finalizing']
    )
    
    response_stream = client.models.generate_content_stream(
        model=model_name,
        contents=main_prompt,
        config=config
    )
    
    for chunk in response_stream:
        yield chunk