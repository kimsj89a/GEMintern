import os
import re
import io
import gc
import math
import json
import shutil
import base64
import tempfile
import subprocess
import time
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Tuple, Iterator

from openai import OpenAI

# Gemini ì§€ì›ì„ ìœ„í•œ ì„ íƒì  import
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


# =========================
# Utilities
# =========================

SUPPORTED_FORMATS = ["flac", "m4a", "mp3", "mp4", "mpeg", "mpga", "oga", "ogg", "wav", "webm"]

def _ext(name: str) -> str:
    return (name.split(".")[-1] if "." in name else "").lower()

def _has_ffmpeg() -> bool:
    return shutil.which("ffmpeg") is not None

def _run(cmd: List[str]) -> None:
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if p.returncode != 0:
        raise RuntimeError(f"Command failed: {' '.join(cmd)}\n{p.stderr[:2000]}")

def _safe_seek(uploaded_file) -> None:
    try:
        uploaded_file.seek(0)
    except Exception:
        pass

def _write_uploaded_to_temp(uploaded_file, suffix: str) -> str:
    """
    Streamlitì˜ UploadedFileì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥.
    íŒŒì¼ ë‚´ìš©ì„ ì½ì€ í›„ ì¦‰ì‹œ í•¸ë“¤ ì •ë¦¬.
    """
    _safe_seek(uploaded_file)
    # íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ë¡œ ì½ê¸°
    file_content = uploaded_file.read()

    # ì„ì‹œ íŒŒì¼ì— ì“°ê¸° (í•¸ë“¤ì„ ì¦‰ì‹œ ë‹«ìŒ)
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        tmp_file.write(file_content)
        tmp_file.flush()
        return tmp_file.name
    finally:
        tmp_file.close()  # ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°

def _get_duration_seconds(path: str) -> Optional[float]:
    # Uses ffprobe via ffmpeg install; if not available, returns None
    if not _has_ffmpeg():
        return None
    # ffprobe might not exist separately; ffmpeg typically ships it, but not always.
    ffprobe = shutil.which("ffprobe")
    if not ffprobe:
        return None
    cmd = [
        ffprobe, "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        path
    ]
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if p.returncode != 0:
        return None
    try:
        return float(p.stdout.strip())
    except Exception:
        return None

def _format_timestamp(t: float) -> str:
    # mm:ss
    m = int(t // 60)
    s = int(t % 60)
    return f"{m:02d}:{s:02d}"

def _convert_to_wav_16k_mono(input_path: str) -> str:
    """
    Convert any supported audio/video container to a consistent PCM WAV 16kHz mono.
    This significantly reduces format-related edge cases.
    """
    if not _has_ffmpeg():
        # If ffmpeg isn't available, we just return the input.
        # Whisper can still handle many formats, but conversion is recommended.
        return input_path

    # ì„ì‹œ íŒŒì¼ í•¸ë“¤ì„ ì¦‰ì‹œ ë‹«ì•„ Windows íŒŒì¼ ì ê¸ˆ ë¬¸ì œ ë°©ì§€
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    out_path = tmp_file.name
    tmp_file.close()

    _run([
        "ffmpeg", "-y",
        "-i", input_path,
        "-vn",
        "-ac", "1",
        "-ar", "16000",
        "-c:a", "pcm_s16le",
        out_path
    ])
    return out_path

def _split_wav_by_duration(input_wav_path: str, chunk_seconds: int = 600) -> List[str]:
    """
    Split WAV into chunks using ffmpeg's segment muxer.
    chunk_seconds default: 10 minutes
    """
    if not _has_ffmpeg():
        # No safe split without ffmpeg/pydub; return as single chunk
        return [input_wav_path]

    out_dir = tempfile.mkdtemp()
    out_pattern = os.path.join(out_dir, "chunk_%03d.wav")

    _run([
        "ffmpeg", "-y",
        "-i", input_wav_path,
        "-f", "segment",
        "-segment_time", str(chunk_seconds),
        "-c", "copy",
        out_pattern
    ])

    # Collect in order
    chunks = []
    i = 0
    while True:
        p = os.path.join(out_dir, f"chunk_{i:03d}.wav")
        if not os.path.exists(p):
            break
        chunks.append(p)
        i += 1

    return chunks if chunks else [input_wav_path]


# =========================
# Whisper transcription (with timestamps)
# =========================

@dataclass
class Segment:
    start: float
    end: float
    text: str
    speaker: Optional[str] = None

def _transcribe_whisper_verbose(
    client: OpenAI,
    file_path: str,
    original_filename: str,
    language: str = "ko"
) -> Dict[str, Any]:
    """
    Calls Whisper with verbose_json to get segments/timestamps.
    IMPORTANT: pass (filename, fileobj) so format detection doesn't fail.
    """
    # íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ë¡œ ì½ì–´ì„œ ì‚¬ìš© (íŒŒì¼ í•¸ë“¤ ëˆ„ìˆ˜ ë°©ì§€)
    with open(file_path, "rb") as f:
        file_content = f.read()

    # io.BytesIOë¡œ íŒŒì¼ ê°ì²´ ìƒì„±í•˜ì—¬ API í˜¸ì¶œ
    file_obj = io.BytesIO(file_content)
    resp = client.audio.transcriptions.create(
        model="whisper-1",
        file=(original_filename, file_obj),  # <-- í•µì‹¬: íŒŒì¼ëª… í¬í•¨
        language=language,
        response_format="verbose_json",
        timestamp_granularities=["segment"],
    )
    file_obj.close()  # ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°

    # SDKê°€ dict/obj í˜•íƒœë¡œ ì˜¬ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ë³€í™˜
    if isinstance(resp, dict):
        return resp
    # pydantic-like object
    try:
        return resp.model_dump()
    except Exception:
        # last resort
        return json.loads(resp.model_dump_json())

def _collect_segments(verbose_json: Dict[str, Any], time_offset: float = 0.0) -> List[Segment]:
    segs = []
    for s in verbose_json.get("segments", []) or []:
        start = float(s.get("start", 0.0)) + time_offset
        end = float(s.get("end", 0.0)) + time_offset
        text = (s.get("text") or "").strip()
        if text:
            segs.append(Segment(start=start, end=end, text=text))
    return segs


def _transcribe_gemini(
    file_path: str,
    api_key: str,
    model: str = "gemini-3-flash-preview",
    language: str = "ko",
    include_timestamps: bool = False,
    start_offset_sec: float = 0.0,
    total_duration_sec: Optional[float] = None,
    batch_size_sec: int = 600,
    batch_threshold_sec: int = 900
) -> Iterator[str]:
    """
    Geminië¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ì „ì‚¬.
    Generator function yielding text chunks.
    """
    if not GEMINI_AVAILABLE:
        raise RuntimeError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeai")

    # Gemini ì„¤ì •
    genai.configure(api_key=api_key)

    # íŒŒì¼ MIME íƒ€ì… ê²°ì •
    ext = os.path.splitext(file_path)[1].lower()
    mime_types = {
        ".mp3": "audio/mp3",
        ".wav": "audio/wav",
        ".m4a": "audio/mp4",
        ".mp4": "audio/mp4",
        ".mpeg": "audio/mpeg",
        ".mpga": "audio/mpeg",
        ".webm": "audio/webm",
        ".ogg": "audio/ogg",
        ".flac": "audio/flac"
    }
    mime_type = mime_types.get(ext, "audio/mpeg")

    # [ë³€ê²½] ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•´ File API ì‚¬ìš© (Base64 ì œí•œ í•´ê²°)
    try:
        uploaded_file = genai.upload_file(file_path, mime_type=mime_type)
        
        # ì²˜ë¦¬ ëŒ€ê¸° (AudioëŠ” ë³´í†µ ì¦‰ì‹œ ì²˜ë¦¬ë˜ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
        while uploaded_file.state.name == "PROCESSING":
            time.sleep(1)
            uploaded_file = genai.get_file(uploaded_file.name)
            
        if uploaded_file.state.name == "FAILED":
            raise ValueError("Gemini File Upload Failed")
            
    except Exception as e:
        raise RuntimeError(f"Gemini íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    # Gemini ëª¨ë¸ ì´ˆê¸°í™”
    gemini_model = genai.GenerativeModel(
        model_name=model,
        system_instruction="ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ìŒì„± ì „ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì „ì‚¬í•˜ê³ , í™”ìë¥¼ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”."
    )

    # [NEW] ê¸´ íŒŒì¼ ìë™ ë°°ì¹˜ ì²˜ë¦¬ (FFmpeg ë¯¸ì„¤ì¹˜ ëŒ€ì‘)
    # 1. ì¬ìƒ ì‹œê°„ í™•ì¸ (ëª¨ë¸ì—ê²Œ ì§ˆì˜)
    total_duration = total_duration_sec or 0.0
    if total_duration <= 0.0:
        try:
            # ??? ???????????
            dur_prompt = "??????????????? ??? ?????'??seconds)' ??????????? ???????? ??? ??? ????????????? (?? 1234.5)"
            dur_resp = gemini_model.generate_content([dur_prompt, uploaded_file])
            # ??? ???
            nums = re.findall(r"[-+]?\d*\.\d+|\d+", dur_resp.text)
            if nums:
                total_duration = float(nums[0])
        except Exception:
            pass # ??? ????? ????????

    # 2. ë°°ì¹˜ ì²˜ë¦¬ (15ë¶„(900ì´ˆ) ì´ˆê³¼ ì‹œ 10ë¶„ ë‹¨ìœ„ ë…¼ë¦¬ì  ë¶„í• )
        if total_duration > batch_threshold_sec:
        current_pos = 0.0
        
        while current_pos < total_duration:
            end_pos = min(current_pos + batch_size_sec, total_duration)
            
            # ì‹œê°„ í¬ë§·íŒ…
            t_start_str = _format_timestamp(current_pos)
            t_end_str = _format_timestamp(end_pos)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚° (ì „ì²´ ì˜¤í”„ì…‹ + í˜„ì¬ ì²­í¬ ì˜¤í”„ì…‹)
            chunk_start_time = start_offset_sec + current_pos
            chunk_start_str = _format_timestamp(chunk_start_time)
            
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸
            batch_prompt = f"""
            ì˜¤ë””ì˜¤ íŒŒì¼ì˜ **{t_start_str} ë¶€í„° {t_end_str} ê¹Œì§€ì˜ êµ¬ê°„ë§Œ** ì „ì‚¬í•´ì£¼ì„¸ìš”.
            
            ìš”êµ¬ì‚¬í•­:
            - ì–¸ì–´: {language}
            - ì§€ì •ëœ êµ¬ê°„ì˜ ë‚´ìš©ë§Œ ë¹ ì§ì—†ì´ ì „ì‚¬ (ì´ì „/ì´í›„ ë‚´ìš© ë¬´ì‹œ)
            - í™”ì ë¶„ë¦¬ (í™”ì 1, í™”ì 2...)
            - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ êµ¬ë¶„
            """
            
            if include_timestamps:
                batch_prompt += f"\n- ê° ë¬¸ì¥ ì‹œì‘ì— [MM:SS] í˜•ì‹ íƒ€ì„ìŠ¤íƒ¬í”„ í•„ìˆ˜\n- íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” **{chunk_start_str}** ë¶€í„° ì‹œì‘í•˜ì—¬ íë¦„ì— ë§ê²Œ ì‘ì„±"
            
            batch_prompt += "\n\nê²°ê³¼ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
            
            try:
                resp = gemini_model.generate_content([batch_prompt, uploaded_file])
                if resp.text:
                    yield resp.text.strip()
            except Exception as e:
                yield f"[{t_start_str}~{t_end_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}]"
            
            current_pos += batch_size_sec
            time.sleep(2) # Rate limit ì™„í™”
            
        try:
            uploaded_file.delete()
        except:
            pass
            
        return

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìš”ì²­ ë¬¸êµ¬
    timestamp_instruction = ""
    if include_timestamps:
        start_time_str = _format_timestamp(start_offset_sec)
        timestamp_instruction = (
            f"- ê° ë¬¸ì¥ ë˜ëŠ” ë°œí™”ì˜ ì‹œì‘ ë¶€ë¶„ì— [MM:SS] í˜•ì‹ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜ë“œì‹œ í‘œì‹œí•´ì£¼ì„¸ìš”.\n"
            f"- ì£¼ì˜: ì´ ì˜¤ë””ì˜¤ í´ë¦½ì€ ì „ì²´ ë…¹ìŒì˜ {start_time_str} ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤. íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‘ì„±í•  ë•Œ {start_time_str}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ì„ ë”í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. (ì˜ˆ: ì‹œì‘ ì§í›„ ë°œí™” -> [{start_time_str}] ...)"
        )

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""
    ì œê³µëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ìŒì„± ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì „ì‚¬í•´ì£¼ì„¸ìš”.

    ìš”êµ¬ì‚¬í•­:
    - ì–¸ì–´: í•œêµ­ì–´
    - ëª¨ë“  ëŒ€í™”ì™€ ë‚´ìš©ì„ ë¹ ì§ì—†ì´ ì „ì‚¬
    - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ êµ¬ë¶„
    - í™”ìê°€ ë°”ë€” ë•Œë§ˆë‹¤ ì¤„ë°”ê¿ˆì„ í•˜ê³ , "í™”ì 1:", "í™”ì 2:" ë“±ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„ (ê°€ëŠ¥í•œ ê²½ìš°)
    {timestamp_instruction}

    ì „ì‚¬ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
    """

    try:
        # API í˜¸ì¶œ (íŒŒì¼ ê°ì²´ ì „ë‹¬)
        response = gemini_model.generate_content([prompt, uploaded_file])
        yield response.text.strip()
    finally:
        # íŒŒì¼ ì‚­ì œ (ë¦¬ì†ŒìŠ¤ ì •ë¦¬)
        try:
            uploaded_file.delete()
        except Exception:
            pass


# =========================
# (1) ë¬¸ë‹¨ ë‹¨ìœ„ ì •ë¦¬: paragraphing
# =========================

def _paragraphize(
    segments: List[Segment],
    gap_threshold: float = 1.2,
    max_chars: int = 140
) -> List[List[Segment]]:
    """
    ê·¸ë£¹ ê¸°ì¤€:
      - ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ~ë‹¤ìŒ ì‹œì‘ gapì´ í¬ë©´ ë¬¸ë‹¨ ë¶„ë¦¬
      - ë¬¸ë‹¨ ëˆ„ì  ê¸€ììˆ˜ê°€ max_chars ë„˜ì–´ê°€ë©´ ë¬¸ë‹¨ ë¶„ë¦¬
    """
    paras: List[List[Segment]] = []
    cur: List[Segment] = []
    cur_chars = 0

    for i, seg in enumerate(segments):
        if not cur:
            cur = [seg]
            cur_chars = len(seg.text)
            continue

        gap = seg.start - cur[-1].end
        if gap >= gap_threshold or cur_chars + len(seg.text) > max_chars:
            paras.append(cur)
            cur = [seg]
            cur_chars = len(seg.text)
        else:
            cur.append(seg)
            cur_chars += len(seg.text) + 1

    if cur:
        paras.append(cur)
    return paras

def _render_paragraphs(paras: List[List[Segment]], include_timestamps: bool = True) -> str:
    lines = []
    for para in paras:
        start = para[0].start
        end = para[-1].end
        text = " ".join(s.text.strip() for s in para).strip()
        if include_timestamps:
            lines.append(f"[{_format_timestamp(start)}â€“{_format_timestamp(end)}] {text}")
        else:
            lines.append(text)
    return "\n\n".join(lines).strip()


# =========================
# (2) í™”ì ë¶„ë¦¬(ê°€ëŠ¥í•œ ì„ ê¹Œì§€): optional diarization
# =========================
"""
Whisper ìì²´ëŠ” "ì§„ì§œ" í™”ìë¶„ë¦¬ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì•„ë˜ëŠ” ì„ íƒ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

ì˜µì…˜ A (ê¶Œì¥): pyannote.audio + HuggingFace í† í°ìœ¼ë¡œ diarization ìˆ˜í–‰ í›„ ì„¸ê·¸ë¨¼íŠ¸ì— speaker ë¼ë²¨ ì •ë ¬
- í™˜ê²½ì— pyannote.audio ì„¤ì¹˜ + HF_TOKEN í•„ìš”

ì˜µì…˜ B (ê¸°ë³¸): diarization ë¶ˆê°€ â†’ ë‹¨ì¼ í™”ì(ë˜ëŠ” ë¯¸í‘œê¸°)
"""

def _try_diarize_with_pyannote(wav_path: str) -> Optional[List[Tuple[float, float, str]]]:
    """
    Returns list of (start, end, speaker_label).
    Requires:
      pip install pyannote.audio
      export HF_TOKEN=...
    """
    hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
    if not hf_token:
        return None

    try:
        from pyannote.audio import Pipeline  # type: ignore
    except Exception:
        return None

    try:
        pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=hf_token)
        diar = pipeline(wav_path)
        turns = []
        for turn, _, speaker in diar.itertracks(yield_label=True):
            turns.append((float(turn.start), float(turn.end), str(speaker)))
        return turns or None
    except Exception:
        return None

def _assign_speakers_to_segments(
    segments: List[Segment],
    diar_turns: List[Tuple[float, float, str]]
) -> List[Segment]:
    """
    Assign speaker by segment midpoint overlapping diarization turns.
    """
    if not diar_turns:
        return segments

    # For speed, keep turns in order
    j = 0
    for seg in segments:
        mid = (seg.start + seg.end) / 2.0
        while j < len(diar_turns) and diar_turns[j][1] < mid:
            j += 1
        # Check current and previous turn for overlap
        cand = []
        if 0 <= j < len(diar_turns):
            cand.append(diar_turns[j])
        if j - 1 >= 0:
            cand.append(diar_turns[j - 1])

        best = None
        best_ov = 0.0
        for (a, b, spk) in cand:
            ov = max(0.0, min(seg.end, b) - max(seg.start, a))
            if ov > best_ov:
                best_ov = ov
                best = spk
        seg.speaker = best if best_ov > 0 else seg.speaker
    return segments

def _render_with_speakers(paras: List[List[Segment]], include_timestamps: bool = True) -> str:
    """
    If speakers exist, each paragraph is prefixed with the dominant speaker label.
    """
    out = []
    for para in paras:
        speaker = None
        # dominant speaker by total overlap length
        counts: Dict[str, float] = {}
        for s in para:
            if s.speaker:
                counts[s.speaker] = counts.get(s.speaker, 0.0) + (s.end - s.start)
        if counts:
            speaker = max(counts.items(), key=lambda x: x[1])[0]

        start = para[0].start
        end = para[-1].end
        text = " ".join(s.text.strip() for s in para).strip()

        prefix = ""
        if speaker:
            prefix += f"{speaker}: "
        if include_timestamps:
            prefix = f"[{_format_timestamp(start)}â€“{_format_timestamp(end)}] " + prefix

        out.append(prefix + text)
    return "\n\n".join(out).strip()


# =========================
# ì¶”ì„ìƒˆ/ì •ë¦¬ (ê¸°ì¡´ ë¡œì§ ê°œì„ )
# =========================

FILLER_PATTERNS = [
    # í•œêµ­ì–´ëŠ” \b ê²½ê³„ê°€ ì•½í•˜ë¯€ë¡œ "ë¬¸ì¥ ì‹œì‘/ê³µë°±/ì¤„ë°”ê¿ˆ" ì¤‘ì‹¬ìœ¼ë¡œ ì œê±°
    r'(^|\s)(ì•„+)(?=\s)',
    r'(^|\s)(ì–´+)(?=\s)',
    r'(^|\s)(ìŒ+)(?=\s)',
    r'(^|\s)(ìœ¼+)(?=\s)',
    r'(^|\s)(ê·¸+)(?=\s)',
    r'(^|\s)(ì €+)(?=\s)',
    r'(^|\s)(ë­+)(?=\s)',
    r'(^|\s)(ì´ì œ)(?=\s)',
    r'(^|\s)(ê·¸ë‹ˆê¹Œ|ê·¸ëŸ¬ë‹ˆê¹Œ)(?=\s)',
    r'(^|\s)(ê·¸ê²Œ|ì €ê¸°)(?=\s)',
    r'(^|\s)(ì¢€|ë§‰|ì•½ê°„)(?=\s)',
]

def _clean_text_basic(text: str) -> str:
    t = text
    for pat in FILLER_PATTERNS:
        t = re.sub(pat, r'\1', t, flags=re.IGNORECASE)
    # ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    t = re.sub(r'[ \t]+', ' ', t)
    t = re.sub(r'\n{3,}', '\n\n', t)
    t = t.strip()
    return t


# =========================
# (4) GPT í›„ì²˜ë¦¬: ìš”ì•½/ì •ë¦¬/ì½”ë”©ìš© ë³€í™˜ ë“±
# =========================

def _gpt_postprocess(
    raw_text: str,
    mode: str = "clean",  # clean | summary | atlas_codebook
    model: str = "gpt-4o-mini",
    api_key: str = None,
    api_type: str = "openai"  # openai | gemini
) -> str:
    """
    mode:
      - clean: ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ë¬¸ì¥ë¶€í˜¸ ìµœì†Œ ì •ë¦¬ + ì˜ë¯¸ ìœ ì§€
      - summary: í•µì‹¬ ìš”ì•½(ë¶ˆë¦¿) + ê²°ì •ì‚¬í•­/ì•¡ì…˜ì•„ì´í…œ
      - atlas_codebook: ì§ˆì ì½”ë”©ìš©(ì£¼ì œ/ì½”ë“œ í›„ë³´) í˜•íƒœë¡œ ì •ë¦¬

    api_type:
      - openai: OpenAI GPT ëª¨ë¸ ì‚¬ìš©
      - gemini: Google Gemini ëª¨ë¸ ì‚¬ìš©
    """
    if mode == "clean":
        instruction = (
            "ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ê³ , "
            "ë„ì–´ì“°ê¸°/ë¬¸ì¥ë¶€í˜¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ê³ , ì¤‘ë³µ í‘œí˜„ì„ ìµœì†Œí™”í•´ ì£¼ì„¸ìš”. "
            "ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
        )
    elif mode == "summary":
        instruction = (
            "ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
            "1) í•µì‹¬ ìš”ì•½(ë¶ˆë¦¿ 5~10ê°œ) "
            "2) ê²°ì •ì‚¬í•­(ìˆìœ¼ë©´) "
            "3) ì•¡ì…˜ì•„ì´í…œ(ë‹´ë‹¹/ê¸°í•œì´ ì–¸ê¸‰ë˜ë©´ í¬í•¨) "
            "í˜•íƒœë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”. ì—†ëŠ” í•­ëª©ì€ 'ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. "
            "ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."
        )
    elif mode == "atlas_codebook":
        instruction = (
            "ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ì§ˆì  ì—°êµ¬ ì½”ë”©ì— ë°”ë¡œ ì“°ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”. "
            "ì¶œë ¥ í˜•ì‹:\n"
            "- ì½”ë“œ í›„ë³´ ëª©ë¡(10~25ê°œ): ì½”ë“œëª… / ì •ì˜ / ì˜ˆì‹œ ì¸ìš©(ì§§ê²Œ)\n"
            "- ì ì • ìƒìœ„ë²”ì£¼(3~7ê°œ): ë²”ì£¼ëª… / í¬í•¨ ì½”ë“œ\n"
            "í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì„ ë§Œë“¤ì§€ ë§ê³ , ì¸ìš©ì€ ì›ë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ì„¸ìš”."
        )
    elif mode == "meeting_summary":
        instruction = (
            "ë‹¤ìŒì€ íšŒì˜ ë…¹ìŒì˜ ì „ì‚¬ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)\n"
            "ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì˜ íšŒì˜ë¡ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            "1. ğŸ“Œ 3ì¤„ í•µì‹¬ ìš”ì•½\n"
            "   - ì „ì²´ íšŒì˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ê²°ë¡ ì´ë‚˜ ë‚´ìš©ì„ 3ê°€ì§€ë¡œ ìš”ì•½ (ê°œì¡°ì‹)\n\n"
            "2. ğŸ“ ìƒì„¸ ìš”ì•½\n"
            "   - ì£¼ìš” ì£¼ì œê°€ ë°”ë€ŒëŠ” êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ ì •ë¦¬\n"
            "   - ê° êµ¬ê°„ì˜ ì‹œì‘ê³¼ ë ì‹œê°„ì„ [mm:ss ~ mm:ss] í˜•ì‹ìœ¼ë¡œ í—¤ë”ì— í‘œì‹œ (ì˜ˆ: [00:00 ~ 05:30] ì£¼ì œ)\n"
            "   - ë‚´ìš©ì€ Q&A í˜•ì‹(Q: ì§ˆë¬¸, A: ë‹µë³€) ë˜ëŠ” í•µì‹¬ ë‚´ìš© ì„œìˆ í˜•ìœ¼ë¡œ ìƒì„¸íˆ ì •ë¦¬\n"
            "   - ì „ì‚¬ëœ ë‚´ìš©ì˜ íŒ©íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ë¬¸ì¥ì€ ê¹”ë”í•˜ê²Œ ë‹¤ë“¬ì„ ê²ƒ"
        )
    else:
        instruction = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ê³  ì •ë¦¬í•´ ì£¼ì„¸ìš”."

    # Gemini ëª¨ë¸ ì‚¬ìš©
    if api_type == "gemini":
        if not GEMINI_AVAILABLE:
            raise RuntimeError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        genai.configure(api_key=api_key)
        gemini_model = genai.GenerativeModel(
            model_name=model,
            system_instruction=instruction
        )
        response = gemini_model.generate_content(raw_text)
        return response.text.strip()

    # OpenAI ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸)
    else:
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": instruction},
                {"role": "user", "content": raw_text},
            ],
        )
        # SDK ë°˜í™˜ ì•ˆì „ ì²˜ë¦¬
        try:
            return resp.choices[0].message.content.strip()
        except Exception:
            return str(resp).strip()


# =========================
# Main API for Streamlit
# =========================

def transcribe_audio(
    uploaded_file,
    api_key: Optional[str] = None,
    language: str = "ko",
    chunk_seconds: int = 600,        # (3) ê¸´ íŒŒì¼ ìë™ ë¶„í• : 10ë¶„ ë‹¨ìœ„ ê¸°ë³¸
    do_diarization: bool = False,    # (2) í™”ì ë¶„ë¦¬ ì‹œë„ (ê¸°ë³¸ False - HF_TOKEN í•„ìš”)
    include_timestamps: bool = True, # (1) ë¬¸ë‹¨ì— íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ
    remove_fillers: bool = True,     # ì¶”ì„ìƒˆ ì œê±°
    gpt_mode: Optional[str] = None,  # (4) "clean"|"summary"|"atlas_codebook"|None
    gpt_model: str = "gpt-5.2",
    engine: str = "whisper",         # ì „ì‚¬ ì—”ì§„: "whisper" | "gemini"
    gemini_model: str = "gemini-3-flash-preview",  # Gemini ëª¨ë¸
) -> Iterator[str]:
    """
    1) ë¬¸ë‹¨ ì •ë¦¬: ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ ë¬¸ë‹¨í™”(íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì˜µì…˜)
    2) í™”ì ë¶„ë¦¬: pyannote ê°€ëŠ¥ ì‹œ ìë™ ë¼ë²¨ë§(ë¶ˆê°€ ì‹œ ë¯¸í‘œê¸°)
    3) ê¸´ íŒŒì¼ ë¶„í• : ffmpeg ìˆìœ¼ë©´ chunk_secondsë¡œ ìë™ ë¶„í•  í›„ í•©ì¹˜ê¸°
    4) GPT í›„ì²˜ë¦¬: clean/summary/atlas_codebook ëª¨ë“œ ì œê³µ
    5) ì—”ì§„ ì„ íƒ: Whisper (OpenAI) ë˜ëŠ” Gemini (Google)
    """
    if uploaded_file is None:
        return

    if not api_key:
        yield "[ì˜¤ë¥˜: API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤]"
        return

    original_filename = getattr(uploaded_file, "name", "audio")
    file_ext = _ext(original_filename)

    if file_ext not in SUPPORTED_FORMATS:
        yield f"[ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_FORMATS)}]"
        return

    temp_in = None
    temp_wav = None
    chunk_paths: List[str] = []
    chunk_dirs: set = set()

    try:
        # 0) ì—…ë¡œë“œ íŒŒì¼ì„ ë¨¼ì € ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (íŒŒì¼ í•¸ë“¤ ì¦‰ì‹œ í•´ì œ)
        temp_in = _write_uploaded_to_temp(uploaded_file, suffix=f".{file_ext}")
        # ì—…ë¡œë“œ íŒŒì¼ í¬ì¸í„°ë¥¼ ë°”ë¡œ ë¦¬ì…‹ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        _safe_seek(uploaded_file)

        # (ì•ˆì •í™”) 16k mono wavë¡œ ë³€í™˜ (ffmpeg ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©)
        # Gemini, Whisper ëª¨ë‘ WAVê°€ ì•ˆì •ì ì´ë©°, ë¶„í• (Chunking)ì„ ìœ„í•´ í•„ìš”í•¨
        temp_wav = _convert_to_wav_16k_mono(temp_in)
        total_duration_sec = _get_duration_seconds(temp_wav)

        # (3) ê¸´ íŒŒì¼ ë¶„í•  (Whisper, Gemini ê³µí†µ ì ìš©)
        chunk_paths = _split_wav_by_duration(temp_wav, chunk_seconds=chunk_seconds)

        # Gemini ì—”ì§„ ì‚¬ìš©
        if engine == "gemini":
            for i, cp in enumerate(chunk_paths):
                offset_sec = i * chunk_seconds
                # _transcribe_gemini is now a generator
                for part_text in _transcribe_gemini(
                    file_path=cp,
                    api_key=api_key,
                    model=gemini_model,
                    language=language,
                    include_timestamps=include_timestamps,
                    start_offset_sec=offset_sec,
                    total_duration_sec=total_duration_sec
                ):
                    yield _clean_text_basic(part_text) if remove_fillers else part_text

        # Whisper ì—”ì§„ ì‚¬ìš© (ê¸°ë³¸)
        else:
            client = OpenAI(api_key=api_key)

            # (2) í™”ì ë¶„ë¦¬ ì‹œë„ (ì „ì²´ íŒŒì¼ì— ëŒ€í•´ ë¨¼ì € ìˆ˜í–‰)
            diar_turns = None
            if do_diarization:
                diar_turns = _try_diarize_with_pyannote(temp_wav)

            offset = 0.0

            # duration ê¸°ë°˜ ì˜¤í”„ì…‹(ê°€ëŠ¥í•˜ë©´ ì •í™•)
            # ffprobeê°€ ì—†ìœ¼ë©´ offsetì€ chunk_secondsë¡œ ê·¼ì‚¬
            for idx, cp in enumerate(chunk_paths):
                # 1) ì²­í¬ ì „ì‚¬
                verbose = _transcribe_whisper_verbose(
                    client=client,
                    file_path=cp,
                original_filename=os.path.basename(cp),  # ì‹¤ì œ íŒŒì¼(ì²­í¬)ì˜ í™•ì¥ìì— ë§ê²Œ ì „ë‹¬ (ì˜¤ë¥˜ í•´ê²°)
                    language=language
                )
                
                # 2) ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘ ë° ì˜¤í”„ì…‹ ì ìš©
                chunk_segments = _collect_segments(verbose, time_offset=offset)
                
                # 3) í™”ì í• ë‹¹ (í•´ë‹¹ êµ¬ê°„ì— ë§ëŠ” diarization ê²°ê³¼ ë§¤í•‘)
                if diar_turns:
                    chunk_segments = _assign_speakers_to_segments(chunk_segments, diar_turns)

                # 4) ë¬¸ë‹¨ ì •ë¦¬ ë° ë Œë”ë§
                paras = _paragraphize(chunk_segments, gap_threshold=1.2, max_chars=160)
                
                if any(s.speaker for s in chunk_segments):
                    chunk_body = _render_with_speakers(paras, include_timestamps=include_timestamps)
                else:
                    chunk_body = _render_paragraphs(paras, include_timestamps=include_timestamps)

                # 5) ê²°ê³¼ Yield
                if remove_fillers:
                    yield _clean_text_basic(chunk_body)
                else:
                    yield chunk_body

                dur = _get_duration_seconds(cp)
                if dur is not None:
                    offset += dur
                else:
                    offset += float(chunk_seconds)

        # End of generator

    except Exception as e:
        yield f"[ì˜¤ë””ì˜¤ ì „ì‚¬ ì˜¤ë¥˜: {original_filename} - {str(e)}]"

    finally:
        # 0. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ íŒŒì¼ í•¸ë“¤ í•´ì œ í™•ì¸
        import time
        gc.collect()
        time.sleep(0.05)  # ì§§ì€ ëŒ€ê¸°ë¡œ OSê°€ í•¸ë“¤ì„ ì •ë¦¬í•  ì‹œê°„ ì œê³µ

        # 1. ì„ì‹œ íŒŒì¼ ì‚­ì œ (ë©”ì¸ ì„ì‹œ íŒŒì¼ë“¤)
        for p in [temp_in, temp_wav]:
            if p and os.path.exists(p):
                try:
                    os.unlink(p)
                except PermissionError:
                    # Windowsì—ì„œ íŒŒì¼ì´ ì•„ì§ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŒ - ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    time.sleep(0.1)
                    try:
                        os.unlink(p)
                    except Exception:
                        pass
                except Exception:
                    pass

        # 2. ì²­í¬ íŒŒì¼ë“¤ ì‚­ì œ ë° ë””ë ‰í† ë¦¬ ìˆ˜ì§‘
        for cp in chunk_paths:
            if cp and os.path.exists(cp):
                # ë””ë ‰í† ë¦¬ ê²½ë¡œ ê¸°ì–µ
                parent_dir = os.path.dirname(cp)
                if parent_dir:
                    chunk_dirs.add(parent_dir)
                # ì²­í¬ íŒŒì¼ ì‚­ì œ
                try:
                    os.unlink(cp)
                except PermissionError:
                    time.sleep(0.1)
                    try:
                        os.unlink(cp)
                    except Exception:
                        pass
                except Exception:
                    pass

        # 3. ì²­í¬ ë””ë ‰í† ë¦¬ ì‚­ì œ
        for d in chunk_dirs:
            if d and os.path.exists(d) and os.path.isdir(d):
                try:
                    # ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                    if not os.listdir(d):
                        os.rmdir(d)
                except Exception:
                    pass

        # 4. ë§ˆì§€ë§‰ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        gc.collect()


def is_audio_file(filename: str) -> bool:
    return _ext(filename) in SUPPORTED_FORMATS
